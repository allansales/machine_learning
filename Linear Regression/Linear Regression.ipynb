{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Exercício 1 referente à disciplina de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modifique o código original para imprimir o RSS a cada iteração do gradiente descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import time\n",
    "\n",
    "def compute_error_for_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m*x+b))**2\n",
    "    return totalError/float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) *  x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "        error = compute_error_for_given_points(b, m, points)\n",
    "        #print \"Error: {0}\".format(error)\n",
    "    return [b,m]\n",
    "\n",
    "def run(learning_rate, num_iterations):\n",
    "    points = genfromtxt('income.csv', delimiter=',')\n",
    "\n",
    "    initial_b = 0\n",
    "    initial_m = 0\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print \"intercept: {0}\".format(b)\n",
    "    print \"slope: {0}\".format(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. O que acontece com o RSS ao longo das iterações (aumenta ou diminui) se você usar 1000 iterações e um learning_rate (tamanho do passo do gradiente) de 0.001? Por que você acha que isso acontece?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 707.27101659\n",
      "Error: 232.508779735\n",
      "Error: 131.846692069\n",
      "Error: 110.494831064\n",
      "Error: 105.956950742\n",
      "Error: 104.983681088\n",
      "Error: 104.766120491\n",
      "Error: 104.708774355\n",
      "Error: 104.685396244\n",
      "Error: 104.669221579\n",
      "Error: 104.654576198\n",
      "Error: 104.640257162\n",
      "Error: 104.626009445\n",
      "Error: 104.61177898\n",
      "Error: 104.597554306\n",
      "Error: 104.58333299\n",
      "Error: 104.569114518\n",
      "Error: 104.554898779\n",
      "Error: 104.540685751\n",
      "Error: 104.526475427\n",
      "Error: 104.512267807\n",
      "Error: 104.498062889\n",
      "Error: 104.483860672\n",
      "Error: 104.469661157\n",
      "Error: 104.455464343\n",
      "Error: 104.441270229\n",
      "Error: 104.427078814\n",
      "Error: 104.412890099\n",
      "Error: 104.398704083\n",
      "Error: 104.384520765\n",
      "Error: 104.370340144\n",
      "Error: 104.356162221\n",
      "Error: 104.341986994\n",
      "Error: 104.327814464\n",
      "Error: 104.313644629\n",
      "Error: 104.299477489\n",
      "Error: 104.285313044\n",
      "Error: 104.271151293\n",
      "Error: 104.256992236\n",
      "Error: 104.242835872\n",
      "Error: 104.2286822\n",
      "Error: 104.21453122\n",
      "Error: 104.200382932\n",
      "Error: 104.186237335\n",
      "Error: 104.172094428\n",
      "Error: 104.157954212\n",
      "Error: 104.143816684\n",
      "Error: 104.129681846\n",
      "Error: 104.115549697\n",
      "Error: 104.101420235\n",
      "Error: 104.087293461\n",
      "Error: 104.073169373\n",
      "Error: 104.059047972\n",
      "Error: 104.044929257\n",
      "Error: 104.030813228\n",
      "Error: 104.016699883\n",
      "Error: 104.002589223\n",
      "Error: 103.988481246\n",
      "Error: 103.974375953\n",
      "Error: 103.960273343\n",
      "Error: 103.946173415\n",
      "Error: 103.932076169\n",
      "Error: 103.917981604\n",
      "Error: 103.90388972\n",
      "Error: 103.889800517\n",
      "Error: 103.875713993\n",
      "Error: 103.861630148\n",
      "Error: 103.847548982\n",
      "Error: 103.833470495\n",
      "Error: 103.819394685\n",
      "Error: 103.805321553\n",
      "Error: 103.791251097\n",
      "Error: 103.777183317\n",
      "Error: 103.763118213\n",
      "Error: 103.749055785\n",
      "Error: 103.734996031\n",
      "Error: 103.720938951\n",
      "Error: 103.706884545\n",
      "Error: 103.692832812\n",
      "Error: 103.678783752\n",
      "Error: 103.664737364\n",
      "Error: 103.650693647\n",
      "Error: 103.636652602\n",
      "Error: 103.622614227\n",
      "Error: 103.608578523\n",
      "Error: 103.594545488\n",
      "Error: 103.580515122\n",
      "Error: 103.566487424\n",
      "Error: 103.552462395\n",
      "Error: 103.538440034\n",
      "Error: 103.524420339\n",
      "Error: 103.510403311\n",
      "Error: 103.496388949\n",
      "Error: 103.482377253\n",
      "Error: 103.468368222\n",
      "Error: 103.454361855\n",
      "Error: 103.440358153\n",
      "Error: 103.426357113\n",
      "Error: 103.412358737\n",
      "Error: 103.398363024\n",
      "Error: 103.384369972\n",
      "Error: 103.370379582\n",
      "Error: 103.356391853\n",
      "Error: 103.342406784\n",
      "Error: 103.328424376\n",
      "Error: 103.314444626\n",
      "Error: 103.300467536\n",
      "Error: 103.286493104\n",
      "Error: 103.272521331\n",
      "Error: 103.258552214\n",
      "Error: 103.244585755\n",
      "Error: 103.230621952\n",
      "Error: 103.216660805\n",
      "Error: 103.202702314\n",
      "Error: 103.188746477\n",
      "Error: 103.174793295\n",
      "Error: 103.160842767\n",
      "Error: 103.146894892\n",
      "Error: 103.13294967\n",
      "Error: 103.1190071\n",
      "Error: 103.105067183\n",
      "Error: 103.091129916\n",
      "Error: 103.077195301\n",
      "Error: 103.063263336\n",
      "Error: 103.049334021\n",
      "Error: 103.035407355\n",
      "Error: 103.021483338\n",
      "Error: 103.00756197\n",
      "Error: 102.993643249\n",
      "Error: 102.979727176\n",
      "Error: 102.96581375\n",
      "Error: 102.95190297\n",
      "Error: 102.937994836\n",
      "Error: 102.924089347\n",
      "Error: 102.910186503\n",
      "Error: 102.896286303\n",
      "Error: 102.882388747\n",
      "Error: 102.868493835\n",
      "Error: 102.854601565\n",
      "Error: 102.840711938\n",
      "Error: 102.826824953\n",
      "Error: 102.812940609\n",
      "Error: 102.799058905\n",
      "Error: 102.785179842\n",
      "Error: 102.771303419\n",
      "Error: 102.757429635\n",
      "Error: 102.74355849\n",
      "Error: 102.729689984\n",
      "Error: 102.715824115\n",
      "Error: 102.701960883\n",
      "Error: 102.688100288\n",
      "Error: 102.67424233\n",
      "Error: 102.660387007\n",
      "Error: 102.64653432\n",
      "Error: 102.632684268\n",
      "Error: 102.618836849\n",
      "Error: 102.604992065\n",
      "Error: 102.591149914\n",
      "Error: 102.577310395\n",
      "Error: 102.563473509\n",
      "Error: 102.549639255\n",
      "Error: 102.535807632\n",
      "Error: 102.52197864\n",
      "Error: 102.508152278\n",
      "Error: 102.494328546\n",
      "Error: 102.480507444\n",
      "Error: 102.46668897\n",
      "Error: 102.452873124\n",
      "Error: 102.439059906\n",
      "Error: 102.425249316\n",
      "Error: 102.411441352\n",
      "Error: 102.397636014\n",
      "Error: 102.383833303\n",
      "Error: 102.370033216\n",
      "Error: 102.356235755\n",
      "Error: 102.342440918\n",
      "Error: 102.328648704\n",
      "Error: 102.314859114\n",
      "Error: 102.301072147\n",
      "Error: 102.287287802\n",
      "Error: 102.273506078\n",
      "Error: 102.259726976\n",
      "Error: 102.245950495\n",
      "Error: 102.232176635\n",
      "Error: 102.218405394\n",
      "Error: 102.204636772\n",
      "Error: 102.190870769\n",
      "Error: 102.177107384\n",
      "Error: 102.163346618\n",
      "Error: 102.149588468\n",
      "Error: 102.135832936\n",
      "Error: 102.122080019\n",
      "Error: 102.108329719\n",
      "Error: 102.094582034\n",
      "Error: 102.080836964\n",
      "Error: 102.067094508\n",
      "Error: 102.053354666\n",
      "Error: 102.039617437\n",
      "Error: 102.025882821\n",
      "Error: 102.012150817\n",
      "Error: 101.998421426\n",
      "Error: 101.984694645\n",
      "Error: 101.970970476\n",
      "Error: 101.957248917\n",
      "Error: 101.943529967\n",
      "Error: 101.929813627\n",
      "Error: 101.916099896\n",
      "Error: 101.902388774\n",
      "Error: 101.888680259\n",
      "Error: 101.874974352\n",
      "Error: 101.861271051\n",
      "Error: 101.847570357\n",
      "Error: 101.833872269\n",
      "Error: 101.820176786\n",
      "Error: 101.806483908\n",
      "Error: 101.792793634\n",
      "Error: 101.779105965\n",
      "Error: 101.765420899\n",
      "Error: 101.751738435\n",
      "Error: 101.738058574\n",
      "Error: 101.724381316\n",
      "Error: 101.710706658\n",
      "Error: 101.697034602\n",
      "Error: 101.683365145\n",
      "Error: 101.669698289\n",
      "Error: 101.656034033\n",
      "Error: 101.642372375\n",
      "Error: 101.628713316\n",
      "Error: 101.615056854\n",
      "Error: 101.601402991\n",
      "Error: 101.587751724\n",
      "Error: 101.574103054\n",
      "Error: 101.560456979\n",
      "Error: 101.546813501\n",
      "Error: 101.533172617\n",
      "Error: 101.519534328\n",
      "Error: 101.505898632\n",
      "Error: 101.492265531\n",
      "Error: 101.478635022\n",
      "Error: 101.465007106\n",
      "Error: 101.451381782\n",
      "Error: 101.437759049\n",
      "Error: 101.424138908\n",
      "Error: 101.410521357\n",
      "Error: 101.396906396\n",
      "Error: 101.383294025\n",
      "Error: 101.369684243\n",
      "Error: 101.356077049\n",
      "Error: 101.342472444\n",
      "Error: 101.328870426\n",
      "Error: 101.315270996\n",
      "Error: 101.301674152\n",
      "Error: 101.288079894\n",
      "Error: 101.274488222\n",
      "Error: 101.260899135\n",
      "Error: 101.247312632\n",
      "Error: 101.233728714\n",
      "Error: 101.220147379\n",
      "Error: 101.206568628\n",
      "Error: 101.192992459\n",
      "Error: 101.179418873\n",
      "Error: 101.165847868\n",
      "Error: 101.152279445\n",
      "Error: 101.138713602\n",
      "Error: 101.12515034\n",
      "Error: 101.111589657\n",
      "Error: 101.098031553\n",
      "Error: 101.084476029\n",
      "Error: 101.070923082\n",
      "Error: 101.057372714\n",
      "Error: 101.043824922\n",
      "Error: 101.030279708\n",
      "Error: 101.01673707\n",
      "Error: 101.003197007\n",
      "Error: 100.98965952\n",
      "Error: 100.976124608\n",
      "Error: 100.96259227\n",
      "Error: 100.949062507\n",
      "Error: 100.935535316\n",
      "Error: 100.922010698\n",
      "Error: 100.908488653\n",
      "Error: 100.89496918\n",
      "Error: 100.881452278\n",
      "Error: 100.867937947\n",
      "Error: 100.854426186\n",
      "Error: 100.840916996\n",
      "Error: 100.827410375\n",
      "Error: 100.813906323\n",
      "Error: 100.800404839\n",
      "Error: 100.786905924\n",
      "Error: 100.773409576\n",
      "Error: 100.759915794\n",
      "Error: 100.74642458\n",
      "Error: 100.732935931\n",
      "Error: 100.719449849\n",
      "Error: 100.705966331\n",
      "Error: 100.692485378\n",
      "Error: 100.679006988\n",
      "Error: 100.665531163\n",
      "Error: 100.652057901\n",
      "Error: 100.638587201\n",
      "Error: 100.625119063\n",
      "Error: 100.611653487\n",
      "Error: 100.598190472\n",
      "Error: 100.584730018\n",
      "Error: 100.571272125\n",
      "Error: 100.55781679\n",
      "Error: 100.544364015\n",
      "Error: 100.530913799\n",
      "Error: 100.517466141\n",
      "Error: 100.504021041\n",
      "Error: 100.490578498\n",
      "Error: 100.477138512\n",
      "Error: 100.463701082\n",
      "Error: 100.450266208\n",
      "Error: 100.43683389\n",
      "Error: 100.423404126\n",
      "Error: 100.409976916\n",
      "Error: 100.396552261\n",
      "Error: 100.383130159\n",
      "Error: 100.36971061\n",
      "Error: 100.356293613\n",
      "Error: 100.342879168\n",
      "Error: 100.329467274\n",
      "Error: 100.316057932\n",
      "Error: 100.30265114\n",
      "Error: 100.289246898\n",
      "Error: 100.275845205\n",
      "Error: 100.262446062\n",
      "Error: 100.249049467\n",
      "Error: 100.23565542\n",
      "Error: 100.222263921\n",
      "Error: 100.208874969\n",
      "Error: 100.195488563\n",
      "Error: 100.182104704\n",
      "Error: 100.16872339\n",
      "Error: 100.155344621\n",
      "Error: 100.141968397\n",
      "Error: 100.128594717\n",
      "Error: 100.115223581\n",
      "Error: 100.101854988\n",
      "Error: 100.088488938\n",
      "Error: 100.07512543\n",
      "Error: 100.061764463\n",
      "Error: 100.048406039\n",
      "Error: 100.035050154\n",
      "Error: 100.021696811\n",
      "Error: 100.008346006\n",
      "Error: 99.9949977418\n",
      "Error: 99.981652016\n",
      "Error: 99.9683088286\n",
      "Error: 99.9549681791\n",
      "Error: 99.9416300669\n",
      "Error: 99.9282944918\n",
      "Error: 99.914961453\n",
      "Error: 99.9016309503\n",
      "Error: 99.888302983\n",
      "Error: 99.8749775507\n",
      "Error: 99.8616546529\n",
      "Error: 99.8483342892\n",
      "Error: 99.835016459\n",
      "Error: 99.8217011619\n",
      "Error: 99.8083883974\n",
      "Error: 99.795078165\n",
      "Error: 99.7817704643\n",
      "Error: 99.7684652946\n",
      "Error: 99.7551626557\n",
      "Error: 99.7418625469\n",
      "Error: 99.7285649679\n",
      "Error: 99.715269918\n",
      "Error: 99.7019773969\n",
      "Error: 99.6886874041\n",
      "Error: 99.675399939\n",
      "Error: 99.6621150012\n",
      "Error: 99.6488325903\n",
      "Error: 99.6355527057\n",
      "Error: 99.6222753469\n",
      "Error: 99.6090005135\n",
      "Error: 99.5957282051\n",
      "Error: 99.582458421\n",
      "Error: 99.5691911609\n",
      "Error: 99.5559264242\n",
      "Error: 99.5426642105\n",
      "Error: 99.5294045193\n",
      "Error: 99.5161473501\n",
      "Error: 99.5028927024\n",
      "Error: 99.4896405758\n",
      "Error: 99.4763909698\n",
      "Error: 99.4631438839\n",
      "Error: 99.4498993176\n",
      "Error: 99.4366572704\n",
      "Error: 99.4234177419\n",
      "Error: 99.4101807316\n",
      "Error: 99.3969462389\n",
      "Error: 99.3837142635\n",
      "Error: 99.3704848049\n",
      "Error: 99.3572578625\n",
      "Error: 99.3440334359\n",
      "Error: 99.3308115246\n",
      "Error: 99.3175921281\n",
      "Error: 99.304375246\n",
      "Error: 99.2911608777\n",
      "Error: 99.2779490229\n",
      "Error: 99.264739681\n",
      "Error: 99.2515328515\n",
      "Error: 99.2383285339\n",
      "Error: 99.2251267279\n",
      "Error: 99.2119274328\n",
      "Error: 99.1987306483\n",
      "Error: 99.1855363739\n",
      "Error: 99.172344609\n",
      "Error: 99.1591553532\n",
      "Error: 99.145968606\n",
      "Error: 99.132784367\n",
      "Error: 99.1196026356\n",
      "Error: 99.1064234114\n",
      "Error: 99.0932466939\n",
      "Error: 99.0800724827\n",
      "Error: 99.0669007772\n",
      "Error: 99.053731577\n",
      "Error: 99.0405648816\n",
      "Error: 99.0274006905\n",
      "Error: 99.0142390033\n",
      "Error: 99.0010798195\n",
      "Error: 98.9879231385\n",
      "Error: 98.97476896\n",
      "Error: 98.9616172834\n",
      "Error: 98.9484681083\n",
      "Error: 98.9353214342\n",
      "Error: 98.9221772606\n",
      "Error: 98.9090355871\n",
      "Error: 98.8958964131\n",
      "Error: 98.8827597382\n",
      "Error: 98.869625562\n",
      "Error: 98.8564938838\n",
      "Error: 98.8433647034\n",
      "Error: 98.8302380201\n",
      "Error: 98.8171138336\n",
      "Error: 98.8039921433\n",
      "Error: 98.7908729487\n",
      "Error: 98.7777562495\n",
      "Error: 98.7646420451\n",
      "Error: 98.751530335\n",
      "Error: 98.7384211188\n",
      "Error: 98.7253143959\n",
      "Error: 98.712210166\n",
      "Error: 98.6991084286\n",
      "Error: 98.6860091831\n",
      "Error: 98.6729124291\n",
      "Error: 98.6598181662\n",
      "Error: 98.6467263938\n",
      "Error: 98.6336371115\n",
      "Error: 98.6205503187\n",
      "Error: 98.6074660151\n",
      "Error: 98.5943842002\n",
      "Error: 98.5813048734\n",
      "Error: 98.5682280344\n",
      "Error: 98.5551536826\n",
      "Error: 98.5420818176\n",
      "Error: 98.5290124388\n",
      "Error: 98.5159455459\n",
      "Error: 98.5028811383\n",
      "Error: 98.4898192156\n",
      "Error: 98.4767597772\n",
      "Error: 98.4637028229\n",
      "Error: 98.4506483519\n",
      "Error: 98.437596364\n",
      "Error: 98.4245468585\n",
      "Error: 98.4114998351\n",
      "Error: 98.3984552933\n",
      "Error: 98.3854132325\n",
      "Error: 98.3723736524\n",
      "Error: 98.3593365524\n",
      "Error: 98.3463019321\n",
      "Error: 98.333269791\n",
      "Error: 98.3202401287\n",
      "Error: 98.3072129446\n",
      "Error: 98.2941882383\n",
      "Error: 98.2811660093\n",
      "Error: 98.2681462572\n",
      "Error: 98.2551289815\n",
      "Error: 98.2421141816\n",
      "Error: 98.2291018572\n",
      "Error: 98.2160920078\n",
      "Error: 98.2030846328\n",
      "Error: 98.1900797319\n",
      "Error: 98.1770773046\n",
      "Error: 98.1640773503\n",
      "Error: 98.1510798686\n",
      "Error: 98.1380848591\n",
      "Error: 98.1250923212\n",
      "Error: 98.1121022546\n",
      "Error: 98.0991146587\n",
      "Error: 98.086129533\n",
      "Error: 98.0731468772\n",
      "Error: 98.0601666906\n",
      "Error: 98.0471889729\n",
      "Error: 98.0342137236\n",
      "Error: 98.0212409422\n",
      "Error: 98.0082706283\n",
      "Error: 97.9953027813\n",
      "Error: 97.9823374009\n",
      "Error: 97.9693744865\n",
      "Error: 97.9564140376\n",
      "Error: 97.9434560538\n",
      "Error: 97.9305005347\n",
      "Error: 97.9175474798\n",
      "Error: 97.9045968885\n",
      "Error: 97.8916487604\n",
      "Error: 97.8787030952\n",
      "Error: 97.8657598922\n",
      "Error: 97.852819151\n",
      "Error: 97.8398808711\n",
      "Error: 97.8269450522\n",
      "Error: 97.8140116936\n",
      "Error: 97.8010807951\n",
      "Error: 97.7881523559\n",
      "Error: 97.7752263758\n",
      "Error: 97.7623028543\n",
      "Error: 97.7493817908\n",
      "Error: 97.7364631849\n",
      "Error: 97.7235470362\n",
      "Error: 97.7106333441\n",
      "Error: 97.6977221083\n",
      "Error: 97.6848133282\n",
      "Error: 97.6719070033\n",
      "Error: 97.6590031333\n",
      "Error: 97.6461017176\n",
      "Error: 97.6332027558\n",
      "Error: 97.6203062473\n",
      "Error: 97.6074121919\n",
      "Error: 97.5945205888\n",
      "Error: 97.5816314378\n",
      "Error: 97.5687447383\n",
      "Error: 97.5558604899\n",
      "Error: 97.5429786921\n",
      "Error: 97.5300993445\n",
      "Error: 97.5172224465\n",
      "Error: 97.5043479977\n",
      "Error: 97.4914759977\n",
      "Error: 97.4786064459\n",
      "Error: 97.465739342\n",
      "Error: 97.4528746854\n",
      "Error: 97.4400124757\n",
      "Error: 97.4271527124\n",
      "Error: 97.414295395\n",
      "Error: 97.4014405231\n",
      "Error: 97.3885880963\n",
      "Error: 97.375738114\n",
      "Error: 97.3628905757\n",
      "Error: 97.3500454812\n",
      "Error: 97.3372028297\n",
      "Error: 97.324362621\n",
      "Error: 97.3115248545\n",
      "Error: 97.2986895297\n",
      "Error: 97.2858566463\n",
      "Error: 97.2730262037\n",
      "Error: 97.2601982014\n",
      "Error: 97.2473726391\n",
      "Error: 97.2345495162\n",
      "Error: 97.2217288323\n",
      "Error: 97.2089105869\n",
      "Error: 97.1960947796\n",
      "Error: 97.1832814098\n",
      "Error: 97.1704704772\n",
      "Error: 97.1576619812\n",
      "Error: 97.1448559214\n",
      "Error: 97.1320522974\n",
      "Error: 97.1192511086\n",
      "Error: 97.1064523547\n",
      "Error: 97.0936560351\n",
      "Error: 97.0808621493\n",
      "Error: 97.068070697\n",
      "Error: 97.0552816777\n",
      "Error: 97.0424950908\n",
      "Error: 97.029710936\n",
      "Error: 97.0169292127\n",
      "Error: 97.0041499205\n",
      "Error: 96.991373059\n",
      "Error: 96.9785986277\n",
      "Error: 96.9658266261\n",
      "Error: 96.9530570537\n",
      "Error: 96.9402899101\n",
      "Error: 96.9275251949\n",
      "Error: 96.9147629075\n",
      "Error: 96.9020030476\n",
      "Error: 96.8892456146\n",
      "Error: 96.8764906081\n",
      "Error: 96.8637380275\n",
      "Error: 96.8509878726\n",
      "Error: 96.8382401428\n",
      "Error: 96.8254948376\n",
      "Error: 96.8127519566\n",
      "Error: 96.8000114993\n",
      "Error: 96.7872734652\n",
      "Error: 96.774537854\n",
      "Error: 96.761804665\n",
      "Error: 96.749073898\n",
      "Error: 96.7363455524\n",
      "Error: 96.7236196277\n",
      "Error: 96.7108961235\n",
      "Error: 96.6981750394\n",
      "Error: 96.6854563748\n",
      "Error: 96.6727401293\n",
      "Error: 96.6600263025\n",
      "Error: 96.6473148939\n",
      "Error: 96.634605903\n",
      "Error: 96.6218993293\n",
      "Error: 96.6091951725\n",
      "Error: 96.5964934321\n",
      "Error: 96.5837941075\n",
      "Error: 96.5710971984\n",
      "Error: 96.5584027042\n",
      "Error: 96.5457106246\n",
      "Error: 96.533020959\n",
      "Error: 96.520333707\n",
      "Error: 96.5076488681\n",
      "Error: 96.4949664419\n",
      "Error: 96.482286428\n",
      "Error: 96.4696088258\n",
      "Error: 96.4569336349\n",
      "Error: 96.4442608548\n",
      "Error: 96.4315904851\n",
      "Error: 96.4189225254\n",
      "Error: 96.4062569751\n",
      "Error: 96.3935938338\n",
      "Error: 96.3809331011\n",
      "Error: 96.3682747765\n",
      "Error: 96.3556188595\n",
      "Error: 96.3429653497\n",
      "Error: 96.3303142466\n",
      "Error: 96.3176655497\n",
      "Error: 96.3050192587\n",
      "Error: 96.292375373\n",
      "Error: 96.2797338922\n",
      "Error: 96.2670948158\n",
      "Error: 96.2544581434\n",
      "Error: 96.2418238746\n",
      "Error: 96.2291920087\n",
      "Error: 96.2165625455\n",
      "Error: 96.2039354844\n",
      "Error: 96.1913108251\n",
      "Error: 96.1786885669\n",
      "Error: 96.1660687095\n",
      "Error: 96.1534512525\n",
      "Error: 96.1408361953\n",
      "Error: 96.1282235375\n",
      "Error: 96.1156132786\n",
      "Error: 96.1030054183\n",
      "Error: 96.090399956\n",
      "Error: 96.0777968912\n",
      "Error: 96.0651962236\n",
      "Error: 96.0525979527\n",
      "Error: 96.0400020779\n",
      "Error: 96.027408599\n",
      "Error: 96.0148175153\n",
      "Error: 96.0022288265\n",
      "Error: 95.989642532\n",
      "Error: 95.9770586315\n",
      "Error: 95.9644771245\n",
      "Error: 95.9518980105\n",
      "Error: 95.939321289\n",
      "Error: 95.9267469597\n",
      "Error: 95.9141750221\n",
      "Error: 95.9016054756\n",
      "Error: 95.8890383199\n",
      "Error: 95.8764735545\n",
      "Error: 95.8639111789\n",
      "Error: 95.8513511927\n",
      "Error: 95.8387935955\n",
      "Error: 95.8262383867\n",
      "Error: 95.8136855659\n",
      "Error: 95.8011351327\n",
      "Error: 95.7885870866\n",
      "Error: 95.7760414272\n",
      "Error: 95.763498154\n",
      "Error: 95.7509572665\n",
      "Error: 95.7384187644\n",
      "Error: 95.7258826471\n",
      "Error: 95.7133489141\n",
      "Error: 95.7008175651\n",
      "Error: 95.6882885996\n",
      "Error: 95.6757620171\n",
      "Error: 95.6632378172\n",
      "Error: 95.6507159995\n",
      "Error: 95.6381965634\n",
      "Error: 95.6256795085\n",
      "Error: 95.6131648343\n",
      "Error: 95.6006525405\n",
      "Error: 95.5881426266\n",
      "Error: 95.575635092\n",
      "Error: 95.5631299364\n",
      "Error: 95.5506271593\n",
      "Error: 95.5381267603\n",
      "Error: 95.5256287388\n",
      "Error: 95.5131330945\n",
      "Error: 95.5006398269\n",
      "Error: 95.4881489356\n",
      "Error: 95.47566042\n",
      "Error: 95.4631742798\n",
      "Error: 95.4506905144\n",
      "Error: 95.4382091235\n",
      "Error: 95.4257301065\n",
      "Error: 95.4132534631\n",
      "Error: 95.4007791928\n",
      "Error: 95.3883072951\n",
      "Error: 95.3758377695\n",
      "Error: 95.3633706157\n",
      "Error: 95.3509058332\n",
      "Error: 95.3384434215\n",
      "Error: 95.3259833802\n",
      "Error: 95.3135257087\n",
      "Error: 95.3010704068\n",
      "Error: 95.2886174739\n",
      "Error: 95.2761669095\n",
      "Error: 95.2637187133\n",
      "Error: 95.2512728847\n",
      "Error: 95.2388294233\n",
      "Error: 95.2263883287\n",
      "Error: 95.2139496005\n",
      "Error: 95.2015132381\n",
      "Error: 95.1890792411\n",
      "Error: 95.1766476091\n",
      "Error: 95.1642183416\n",
      "Error: 95.1517914381\n",
      "Error: 95.1393668983\n",
      "Error: 95.1269447217\n",
      "Error: 95.1145249077\n",
      "Error: 95.1021074561\n",
      "Error: 95.0896923662\n",
      "Error: 95.0772796378\n",
      "Error: 95.0648692702\n",
      "Error: 95.0524612631\n",
      "Error: 95.0400556161\n",
      "Error: 95.0276523286\n",
      "Error: 95.0152514003\n",
      "Error: 95.0028528306\n",
      "Error: 94.9904566192\n",
      "Error: 94.9780627655\n",
      "Error: 94.9656712692\n",
      "Error: 94.9532821297\n",
      "Error: 94.9408953467\n",
      "Error: 94.9285109197\n",
      "Error: 94.9161288482\n",
      "Error: 94.9037491318\n",
      "Error: 94.89137177\n",
      "Error: 94.8789967625\n",
      "Error: 94.8666241087\n",
      "Error: 94.8542538082\n",
      "Error: 94.8418858605\n",
      "Error: 94.8295202652\n",
      "Error: 94.8171570219\n",
      "Error: 94.8047961302\n",
      "Error: 94.7924375894\n",
      "Error: 94.7800813993\n",
      "Error: 94.7677275594\n",
      "Error: 94.7553760692\n",
      "Error: 94.7430269282\n",
      "Error: 94.7306801361\n",
      "Error: 94.7183356924\n",
      "Error: 94.7059935966\n",
      "Error: 94.6936538482\n",
      "Error: 94.681316447\n",
      "Error: 94.6689813923\n",
      "Error: 94.6566486837\n",
      "Error: 94.6443183209\n",
      "Error: 94.6319903033\n",
      "Error: 94.6196646306\n",
      "Error: 94.6073413022\n",
      "Error: 94.5950203177\n",
      "Error: 94.5827016767\n",
      "Error: 94.5703853787\n",
      "Error: 94.5580714233\n",
      "Error: 94.54575981\n",
      "Error: 94.5334505384\n",
      "Error: 94.5211436081\n",
      "Error: 94.5088390185\n",
      "Error: 94.4965367694\n",
      "Error: 94.4842368601\n",
      "Error: 94.4719392903\n",
      "Error: 94.4596440595\n",
      "Error: 94.4473511673\n",
      "Error: 94.4350606132\n",
      "Error: 94.4227723968\n",
      "Error: 94.4104865176\n",
      "Error: 94.3982029752\n",
      "Error: 94.3859217692\n",
      "Error: 94.3736428991\n",
      "Error: 94.3613663644\n",
      "Error: 94.3490921648\n",
      "Error: 94.3368202997\n",
      "Error: 94.3245507688\n",
      "Error: 94.3122835715\n",
      "Error: 94.3000187075\n",
      "Error: 94.2877561763\n",
      "Error: 94.2754959775\n",
      "Error: 94.2632381105\n",
      "Error: 94.250982575\n",
      "Error: 94.2387293706\n",
      "Error: 94.2264784967\n",
      "Error: 94.214229953\n",
      "Error: 94.2019837389\n",
      "Error: 94.1897398541\n",
      "Error: 94.1774982981\n",
      "Error: 94.1652590705\n",
      "Error: 94.1530221708\n",
      "Error: 94.1407875986\n",
      "Error: 94.1285553534\n",
      "Error: 94.1163254348\n",
      "Error: 94.1040978424\n",
      "Error: 94.0918725756\n",
      "Error: 94.0796496342\n",
      "Error: 94.0674290175\n",
      "Error: 94.0552107253\n",
      "Error: 94.042994757\n",
      "Error: 94.0307811121\n",
      "Error: 94.0185697904\n",
      "Error: 94.0063607912\n",
      "Error: 93.9941541142\n",
      "Error: 93.981949759\n",
      "Error: 93.969747725\n",
      "Error: 93.9575480119\n",
      "Error: 93.9453506192\n",
      "Error: 93.9331555465\n",
      "Error: 93.9209627932\n",
      "Error: 93.9087723591\n",
      "Error: 93.8965842436\n",
      "Error: 93.8843984463\n",
      "Error: 93.8722149668\n",
      "Error: 93.8600338046\n",
      "Error: 93.8478549592\n",
      "Error: 93.8356784303\n",
      "Error: 93.8235042174\n",
      "Error: 93.8113323201\n",
      "Error: 93.7991627378\n",
      "Error: 93.7869954703\n",
      "Error: 93.7748305169\n",
      "Error: 93.7626678774\n",
      "Error: 93.7505075512\n",
      "Error: 93.738349538\n",
      "Error: 93.7261938372\n",
      "Error: 93.7140404484\n",
      "Error: 93.7018893713\n",
      "Error: 93.6897406052\n",
      "Error: 93.67759415\n",
      "Error: 93.6654500049\n",
      "Error: 93.6533081698\n",
      "Error: 93.641168644\n",
      "Error: 93.6290314272\n",
      "Error: 93.6168965189\n",
      "Error: 93.6047639187\n",
      "Error: 93.5926336261\n",
      "Error: 93.5805056407\n",
      "Error: 93.5683799621\n",
      "Error: 93.5562565898\n",
      "Error: 93.5441355234\n",
      "Error: 93.5320167624\n",
      "Error: 93.5199003065\n",
      "Error: 93.5077861551\n",
      "Error: 93.4956743079\n",
      "Error: 93.4835647643\n",
      "Error: 93.471457524\n",
      "Error: 93.4593525865\n",
      "Error: 93.4472499514\n",
      "Error: 93.4351496183\n",
      "Error: 93.4230515866\n",
      "Error: 93.410955856\n",
      "Error: 93.3988624261\n",
      "Error: 93.3867712963\n",
      "Error: 93.3746824663\n",
      "Error: 93.3625959356\n",
      "Error: 93.3505117038\n",
      "Error: 93.3384297704\n",
      "Error: 93.326350135\n",
      "Error: 93.3142727972\n",
      "Error: 93.3021977565\n",
      "Error: 93.2901250125\n",
      "Error: 93.2780545648\n",
      "Error: 93.2659864129\n",
      "Error: 93.2539205564\n",
      "Error: 93.2418569948\n",
      "Error: 93.2297957277\n",
      "Error: 93.2177367547\n",
      "Error: 93.2056800754\n",
      "Error: 93.1936256892\n",
      "Error: 93.1815735958\n",
      "Error: 93.1695237947\n",
      "Error: 93.1574762856\n",
      "Error: 93.1454310678\n",
      "Error: 93.1333881411\n",
      "Error: 93.121347505\n",
      "Error: 93.1093091591\n",
      "Error: 93.0972731028\n",
      "Error: 93.0852393358\n",
      "Error: 93.0732078577\n",
      "Error: 93.061178668\n",
      "Error: 93.0491517662\n",
      "Error: 93.037127152\n",
      "Error: 93.0251048249\n",
      "Error: 93.0130847845\n",
      "Error: 93.0010670303\n",
      "Error: 92.9890515618\n",
      "Error: 92.9770383788\n",
      "Error: 92.9650274807\n",
      "Error: 92.953018867\n",
      "Error: 92.9410125375\n",
      "Error: 92.9290084915\n",
      "Error: 92.9170067288\n",
      "Error: 92.9050072487\n",
      "Error: 92.8930100511\n",
      "Error: 92.8810151353\n",
      "Error: 92.8690225009\n",
      "Error: 92.8570321476\n",
      "Error: 92.8450440749\n",
      "Error: 92.8330582823\n",
      "Error: 92.8210747694\n",
      "Error: 92.8090935358\n",
      "Error: 92.7971145811\n",
      "Error: 92.7851379048\n",
      "Error: 92.7731635064\n",
      "Error: 92.7611913857\n",
      "Error: 92.749221542\n",
      "Error: 92.737253975\n",
      "Error: 92.7252886843\n",
      "Error: 92.7133256694\n",
      "Error: 92.7013649299\n",
      "Error: 92.6894064653\n",
      "Error: 92.6774502752\n",
      "Error: 92.6654963593\n",
      "Error: 92.653544717\n",
      "Error: 92.6415953479\n",
      "Error: 92.6296482516\n",
      "Error: 92.6177034276\n",
      "Error: 92.6057608756\n",
      "Error: 92.5938205951\n",
      "Error: 92.5818825857\n",
      "Error: 92.5699468468\n",
      "Error: 92.5580133782\n",
      "Error: 92.5460821793\n",
      "Error: 92.5341532498\n",
      "Error: 92.5222265892\n",
      "Error: 92.510302197\n",
      "Error: 92.4983800729\n",
      "Error: 92.4864602164\n",
      "Error: 92.474542627\n",
      "Error: 92.4626273045\n",
      "Error: 92.4507142482\n",
      "Error: 92.4388034578\n",
      "Error: 92.4268949329\n",
      "Error: 92.4149886729\n",
      "Error: 92.4030846776\n",
      "Error: 92.3911829465\n",
      "Error: 92.379283479\n",
      "Error: 92.3673862749\n",
      "Error: 92.3554913336\n",
      "Error: 92.3435986548\n",
      "Error: 92.331708238\n",
      "Error: 92.3198200827\n",
      "Error: 92.3079341886\n",
      "Error: 92.2960505552\n",
      "Error: 92.2841691821\n",
      "Error: 92.2722900689\n",
      "Error: 92.2604132151\n",
      "Error: 92.2485386203\n",
      "Error: 92.236666284\n",
      "Error: 92.2247962059\n",
      "Error: 92.2129283855\n",
      "Error: 92.2010628224\n",
      "Error: 92.1891995161\n",
      "Error: 92.1773384663\n",
      "Error: 92.1654796724\n",
      "Error: 92.1536231341\n",
      "Error: 92.141768851\n",
      "Error: 92.1299168225\n",
      "Error: 92.1180670483\n",
      "Error: 92.106219528\n",
      "Error: 92.0943742611\n",
      "Error: 92.0825312472\n",
      "Error: 92.0706904858\n",
      "Error: 92.0588519766\n",
      "Error: 92.047015719\n",
      "Error: 92.0351817128\n",
      "Error: 92.0233499574\n",
      "Error: 92.0115204524\n",
      "Error: 91.9996931974\n",
      "Error: 91.987868192\n",
      "Error: 91.9760454357\n",
      "Error: 91.9642249281\n",
      "Error: 91.9524066688\n",
      "Error: 91.9405906574\n",
      "Error: 91.9287768933\n",
      "Error: 91.9169653763\n",
      "Error: 91.9051561059\n",
      "Error: 91.8933490815\n",
      "Error: 91.8815443029\n",
      "Error: 91.8697417696\n",
      "Error: 91.8579414812\n",
      "Error: 91.8461434372\n",
      "Error: 91.8343476372\n",
      "Error: 91.8225540808\n",
      "intercept: -3.40347631759\n",
      "slope: 3.453929781\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_iterations = 1000\n",
    "run(learning_rate, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O RSS diminui com o passar das iterações porque os pesos de w0 e w1 vão se ajustando aos dados de forma a se aproximar da função que gerou os pontos. Para qualquer valor de learning rate a tendência de decréssimo do erro se manterá, mas para os diferentes valores de learning rate, a velocidade de decréssimo varia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Teste valores diferentes do número de iterações e learning_rate até que w0 e w1 sejam aproximadamente iguais a -39 e 5 respectivamente. Reporte os valores do número de iterações e learning_rate usados para atingir esses valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: -39.0136399765\n",
      "slope: 5.57373008549\n",
      "--- 2.04008293152 seconds ---\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_iterations = 47500\n",
    "start_time = time.time()\n",
    "run(learning_rate, num_iterations)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: -39.0378764492\n",
      "slope: 5.5751728332\n",
      "--- 0.585549116135 seconds ---\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0037\n",
    "num_iterations = 13000\n",
    "start_time = time.time()\n",
    "run(learning_rate, num_iterations)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se fixamos o learning_rate em 0.001 e aumentamos o valor de num_iterations esperando a convergência do erro e a definição de valores de w0 e w1 de acordo com o pedido, o tempo que levará para a regressão estar pronta é de 2.04 segundos. Dessa forma, pode ser que valha a pena aumentar o learning_rate e tentar diminuir o tempo de processamento da função. \n",
    "\n",
    "Testando o learning_rate com valor de 0.0037, apenas 13000 iterações são suficientes para proporcionar o algoritmo a atingir os valores de -39 e 5. E isso em 0.58 segundos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. O algoritmo do vídeo usa o número de iterações como critério de parada. Mude o algoritmo para considerar um critério de tolerância que é comparado ao tamanho do gradiente (como no algoritmo dos slides apresentados em sala). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import time\n",
    "\n",
    "def compute_error_for_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m*x+b))**2\n",
    "    return totalError/float(len(points))\n",
    "\n",
    "def vector_size(b, m):\n",
    "    return sqrt(b**2 + m**2)\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) *  x * (y - ((m_current * x) + b_current))\n",
    "    norma = vector_size(b_gradient, m_gradient)\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m, norma]\n",
    "\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, error_tolerance):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    norma = float(\"inf\")\n",
    "\n",
    "    while norma > error_tolerance:\n",
    "        b, m, norma = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b,m]\n",
    "\n",
    "def run(learning_rate, error_tolerance):\n",
    "    points = genfromtxt('income.csv', delimiter=',')\n",
    "\n",
    "    initial_b = 0\n",
    "    initial_m = 0\n",
    "\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, error_tolerance)\n",
    "    print \"intercept: {0}\".format(b)\n",
    "    print \"slope: {0}\".format(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ache um valor de tolerância que se aproxime dos valores dos parâmetros do item 4 acima. Que valor foi esse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: -39.0264886326\n",
      "slope: 5.57449493974\n",
      "--- 1.34899401665 seconds ---\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "error_tolerance = 0.04\n",
    "start_time = time.time()\n",
    "run(learning_rate, error_tolerance)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: -39.0266579016\n",
      "slope: 5.57450501598\n",
      "--- 0.361401081085 seconds ---\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0037\n",
    "error_tolerance = 0.04\n",
    "    \n",
    "start_time = time.time()\n",
    "run(learning_rate, error_tolerance)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O valor escolhido é de 0.04. Mas ainda pensando no tempo de processamento do gradiente, há de se notar que a implementação utilizando a tolerância de erro é mais vantajosa que a utilizando um número de iterações, visto que para encontrar valores similares de w0 e w1, as duas configurações de gradiente com contagem de iterações levaram respectivamente 2.04 e 0.58 segundos, enquanto as duas configurações utilizando tolerância de erro tomaram 1.34 e 0.36 segundos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Implemente a forma fechada (equações normais) de calcular os coeficientes de regressão (vide algoritmo nos slides). Compare o tempo de processamento com o gradiente descendente considerando sua solução do item 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "def regression_parameters_calculator(points):\n",
    "    x = points[:,0]\n",
    "    y = points[:,1]\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    m = sum((x - x_mean)*(y - y_mean))/sum((x - x_mean)**2)\n",
    "    b = y_mean-(m*x_mean)\n",
    "    return [m, b]\n",
    "\n",
    "def run():\n",
    "    points = genfromtxt('income.csv', delimiter=',')\n",
    "\n",
    "    slope, intercept = regression_parameters_calculator(points)\n",
    "    \n",
    "    print \"slope: {0}\".format(slope)\n",
    "    print \"intercept: {0}\".format(intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope: 5.59948287412\n",
      "intercept: -39.4462566791\n",
      "--- 0.00115299224854 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "run()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em comparação com o gradiente, a fórmula fechada se mostrou mais rápida (0.00s). Isso é algo esperado de acontecer na regressão simples, dado que a computação de uma equação de forma direta é computacionalmente mais barato que realizar algumas iterações."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
