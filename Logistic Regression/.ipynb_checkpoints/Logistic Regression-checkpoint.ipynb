{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename classes and change values \"Iris-versicolor\" and \"Iris-virginica\" to \"versicolor/virginica\" and, also, add column iris_class that maps setosa to 1 and versicolor/virginica to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = pd.read_csv('iris.data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.loc[(iris[\"iris_class\"] == \"Iris-setosa\"), \"iris_class\"] = \"setosa\"\n",
    "iris.loc[(iris[\"iris_class\"] == \"Iris-versicolor\") | (iris[\"iris_class\"] == \"Iris-virginica\"), \"iris_class\"] = \"versicolor/virginica\"\n",
    "\n",
    "iris.loc[(iris[\"iris_class\"] == \"versicolor/virginica\"), \"iris_labels\"] = 0\n",
    "iris.loc[(iris[\"iris_class\"] == \"setosa\"), \"iris_labels\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_class = iris.iris_class\n",
    "y_labels = iris.iris_labels\n",
    "X = iris.drop([\"iris_class\",\"iris_labels\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1e+15, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = linear_model.LogisticRegression(C=1e15)\n",
    "clf.fit(X, y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Scikit regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.51526204  4.92414975 -7.80941844 -3.81889579]]\n",
      "[ 0.90113843]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coef_)\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized logistic regression code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vector_norm(w):\n",
    "    return np.sqrt(np.sum(w**2))\n",
    "\n",
    "def RSS(w, H, y):\n",
    "    x = (y - H.dot(w))\n",
    "    return(x.transpose().dot(x))\n",
    "\n",
    "def sigmoid(w_current, feature):\n",
    "    mult = np.dot(feature, w_current)\n",
    "    mult = mult.astype(float)\n",
    "    exponent = np.exp(-1*mult)\n",
    "    sig = 1./(1+exponent)\n",
    "    return sig\n",
    "\n",
    "def gradient(target, w_current, feature):\n",
    "    sig = sigmoid(w_current, feature)\n",
    "    mult = (target - sig)[:, np.newaxis]*feature\n",
    "    \n",
    "    w_gradient = np.sum(mult, axis=0)\n",
    "    return w_gradient\n",
    "\n",
    "def step_gradient(feature, target, w_current, learningRate):\n",
    "    \n",
    "    w_gradient = gradient(target, w_current, feature)\n",
    "    new_w = w_current + (learningRate * w_gradient)\n",
    "    \n",
    "    norma = vector_norm(w_gradient)\n",
    "    rss = RSS(new_w, feature, target)\n",
    "    return [new_w, norma, rss]\n",
    "\n",
    "def gradient_descent_runner(feature, target, starting_w, learning_rate, error_tolerance):\n",
    "    w = starting_w\n",
    "    norma = float(\"inf\")\n",
    "    rss_history = []\n",
    "    norma_history = []\n",
    "    \n",
    "    it = 0\n",
    "    while norma > error_tolerance:\n",
    "        it += 1\n",
    "        w, norma, rss = step_gradient(feature, target, w, learning_rate)\n",
    "        if it%10 == 0:\n",
    "            norma_history.append(norma)\n",
    "            rss_history.append(rss)\n",
    "    return [w, norma_history, rss_history]\n",
    "\n",
    "def train_model(feature, target, initial_w, learning_rate, error_tolerance):\n",
    "    w, norma_history, rss_history = gradient_descent_runner(feature, target, initial_w, learning_rate, error_tolerance)\n",
    "    return [w, norma_history, rss_history]\n",
    "\n",
    "# randomly splits instances in training and test sets\n",
    "def train_test_split(points, split_percent = 1):\n",
    "    n = points.shape[1]\n",
    "    initial_w = np.zeros(n)\n",
    "    \n",
    "    N = len(points)\n",
    "    points = np.c_[np.ones(N), points]\n",
    "    \n",
    "    rows_ = np.random.rand(N) < split_percent\n",
    "    train = points[rows_]\n",
    "    test = points[~rows_]\n",
    "    \n",
    "    feature_train = train[:,0:n]\n",
    "    target_train = train[:, n]\n",
    "    \n",
    "    feature_test = test[:,0:n]\n",
    "    target_test = test[:, n]\n",
    "    \n",
    "    return [feature_train, target_train, feature_test, target_test, initial_w]\n",
    "\n",
    "def predict(feature_test, w):\n",
    "    score = np.sum(feature_test * w, axis = 1)\n",
    "    prob = sigmoid(w, feature_test)\n",
    "    \n",
    "    return np.around(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8.32825493813 seconds ---\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0053\n",
    "error_tolerance = 0.001\n",
    "\n",
    "data = iris.drop([\"iris_class\"], axis = 1)\n",
    "feature_train, target_train, feature_test, target_test, initial_w = train_test_split(data)\n",
    "\n",
    "start_time = time.time()\n",
    "w, norma_history, rss_history = train_model(feature_train, target_train, initial_w, learning_rate, error_tolerance)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression coefficients: [ 0.98421228  1.6321127   5.21835306 -8.47674735 -4.2136109 ]\n"
     ]
    }
   ],
   "source": [
    "print \"Logistic regression coefficients: {0}\".format(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, now, can observe that our coefficients are close from scikit's. This might be understood as an evidence of good functioning of our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Predicting using the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.75198793411 seconds ---\n"
     ]
    }
   ],
   "source": [
    "split_percent = 0.75\n",
    "data_2 = iris.drop([\"iris_class\"], axis = 1)\n",
    "feature_train, target_train, feature_test, target_test, initial_w = train_test_split(data_2, split_percent)\n",
    "\n",
    "start_time = time.time()\n",
    "w, norma_history, rss_history = train_model(feature_train, target_train, initial_w, learning_rate, error_tolerance)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression coefficients for 0.75 training data: [ 1.01082759  1.66169782  4.98589465 -8.29486896 -4.27124417]\n"
     ]
    }
   ],
   "source": [
    "print \"Logistic regression coefficients for {0} training data: {1}\".format(split_percent, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the model is well fitted, we ran a simple example using a slice of our dataset to train a model and the remaining instances of it to validate the obtained model. The following confusion matrix shows that our model is predicting all instances of iris correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0  0.0  1.0\n",
      "row_0          \n",
      "0.0     21    0\n",
      "1.0      0   12\n"
     ]
    }
   ],
   "source": [
    "predicted = predict(feature_test, w)\n",
    "print pd.crosstab(target_test, predicted)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
